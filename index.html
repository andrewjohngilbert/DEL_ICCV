<!DOCTYPE html>
<html>
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-81D6829LG0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-81D6829LG0');
</script>

<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding",
    "author": [
      { "@type": "Person", "name": "Mona Ahmadian" },
      { "@type": "Person", "name": "Amir Shirian" },
      { "@type": "Person", "name": "Frank Guerin" },
      { "@type": "Person", "name": "Andrew Gilbert" }
    ],
    "datePublished": "2025-10-01",
    "publisher": {
      "@type": "Organization",
      "name": "International Conference on Computer Vision (ICCV) - MMFM4 Workshop"
    },
    "url": "https://andrewjohngilbert.github.io/DEL_ICCV/",
    "image": "assets/MMKM_Del_Teaser.jpg",
    "description": "We introduce DEL, a framework for dense semantic action localization, aiming to accurately detect and classify multiple actions at fine-grained temporal resolutions in long untrimmed videos. DEL leverages masked self-attention for intra-mode consistency and a multimodal interaction refinement module for cross-modal dependencies. Results on multiple real-world TAL datasets demonstrate more accurate analysis of complex, real-world scenes."
  }
</script>

<div class="social-share">
  <p>Share this research:</p>
  <a href="https://twitter.com/intent/tweet?url=https://andrewjohngilbert.github.io/DEL_ICCV/&text=Check out DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding!" target="_blank" class="button is-small is-info">
    <i class="fab fa-twitter"></i> Twitter
  </a>
  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://andrewjohngilbert.github.io/DEL_ICCV/" target="_blank" class="button is-small is-link">
    <i class="fab fa-linkedin"></i> LinkedIn
  </a>
</div>

<meta charset="utf-8">
<meta name="description" content="DEL: A framework for dense semantic action localization in long untrimmed videos. Results on UnAV-100, THUMOS14, ActivityNet 1.3, and EPIC-Kitchens-100.">
<meta property="og:title" content="DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding"/>
<meta property="og:description" content="DEL accurately detects and classifies multiple actions at fine-grained temporal resolutions in long untrimmed videos using multimodal interaction."/>
<meta property="og:url" content="https://andrewjohngilbert.github.io/DEL_ICCV/"/>
<meta property="og:image" content="assets/MMKM_Del_Teaser.jpg" />
<meta property="og:image:width" content="1200"/>
<meta property="og:image:height" content="630"/>
<meta name="twitter:title" content="DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding">
<meta name="twitter:description" content="DEL: A framework for dense semantic action localization in long untrimmed videos.">
<meta name="twitter:image" content="assets/MMKM_Del_Teaser.jpg">
<meta name="twitter:card" content="summary_large_image">
<!-- Keywords for your paper to be indexed by-->
<meta name="keywords" content="Machine Learning, Video Understanding, Action Localization, Multimodal, Audio-Visual, DEL, Mona Ahmadian, Amir Shirian, Frank Guerin, Andrew Gilbert, ICCV 2025, MMFM4, Temporal Action Localization, UnAV-100, THUMOS14, ActivityNet, EPIC-Kitchens-100">
<meta name="author" content="Mona Ahmadian, Amir Shirian, Frank Guerin, Andrew Gilbert">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding</title>
<link rel="icon" type="image/x-icon" href="static/images/favicon-32x32.png">
<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
rel="stylesheet">

<link rel="stylesheet" href="static/css/bulma.min.css">
<link rel="stylesheet" href="static/css/bulma-carousel.min.css">
<link rel="stylesheet" href="static/css/bulma-slider.min.css">
<link rel="stylesheet" href="static/css/fontawesome.all.min.css">
<link rel="stylesheet"
href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="static/css/index.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
<script defer src="static/js/fontawesome.all.min.js"></script>
<script src="static/js/bulma-carousel.min.js"></script>
<script src="static/js/bulma-slider.min.js"></script>
<script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding</h1>
          <div class="is-size-5 publication-authors">
  <span class="author-block"><a href="https://www.linkedin.com/in/mona-ahmadian-57853521a/" target="_blank">Mona Ahmadian</a>, University of Surrey, m.ahmadian@surrey.ac.uk</span><br>
  <span class="author-block"><a href="https://uk.linkedin.com/in/amir-shirian-6675587b" target="_blank">Amir Shirian</a>, JPMorgan Chase, amirdonte15@gmail.com</span><br>
  <span class="author-block"><a href="https://www.surrey.ac.uk/people/frank-guerin" target="_blank">Frank Guerin</a>, University of Surrey, f.guerin@surrey.ac.uk</span><br>
  <span class="author-block"><a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a>, University of Surrey, a.gilbert@surrey.ac.uk</span>
</div>
<div class="is-size-5 publication-authors">
  <span class="author-block">
    <a href="https://iccv2025.thecvf.com/" target="_blank">International Conference on Computer Vision (ICCV'25)</a> - 
  <a href="https://sites.google.com/view/mmfm4thworkshop/" target="_blank">The 4th Workshop on What is Next in Multimodal Foundation Models? (MMFM4)</a>
  </span>
</div>

              <div class="column has-text-centered">
                <div class="publication-links">
                     <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a href="https://andrewjohngilbert.github.io/DEL_ICCV/assets/MMKM_Del_Paper.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Download Paper (PDF)</span>
                  </a>
                </span>

                <!-- Presentation PDF link -->
               
                <!--
               <span class="link-block">
                  <a href="https://andrewjohngilbert.github.io/DEL_ICCV/assets/HumanvsMachineMindsSpotlight.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Spotlight</span>
                </a>
              </span>  
            -->

                <!-- Poster PDF link -->
                <!-- 
               <span class="link-block">
                  <a href="https://andrewjohngilbert.github.io/DEL_ICCV/assets/HumanvsMachineMindsPoster.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
            -->

            
            
              
                <!-- Github link -->
                 <!--
              <span class="link-block">
                  <a href="https://github.com/SadeghRahmaniB/Epic-ReduAct" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code & Dataset</span>
                </a>
              </span>
            -->

                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/MMKM_Del_Teaser.jpg" alt="Research pipeline for comparing human and AI performance">
      <h2 class="subtitle has-text-centered">
        Real-world videos often feature overlapping events of different lengths, making localization difficult. This image compares ground-truth (GT) with predictions from DEL, an audio-only model (A), and a visual-only model (V). While A and V struggle with a specific category, DEL accurately detects both short and long events, even when overlapping.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser Image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Real-world videos often exhibit overlapping events and intricate temporal dependencies, posing significant challenges for effective multimodal interaction modeling. We introduce DEL, a framework for dense semantic action localization, aiming to accurately detect and classify multiple actions at fine-grained temporal resolutions in long untrimmed videos. DEL consists of two key modules: the alignment of audio and visual features, which leverages masked self-attention to enhance intra-mode consistency, and a multimodal interaction refinement module that models cross-modal dependencies across multiple scales, enabling both high-level semantics and fine-grained details. We report results on multiple real-world Temporal Action Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and EPIC-Kitchens-100. The source code will be made publicly available. These advances enable more accurate analysis of complex, real-world scenes, from surveillance to accessible media understanding.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Model-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Overview of our proposed DEL framework</h2>
      <h2 class="subtitle has-text-centered">
         Our model integrates 1, an adaptive attention mechanism for aligning audio and visual features, 2, inter- and intra-sample contrastive learning to enhance event discrimination, and 3, a multi-scale path aggregation network for feature fusion. DEL efficiently localizes fine-grained and overlapping events in untrimmed videos, leveraging cross-modal dependencies for improved accuracy.
      </h2>
      <img src="assets/MMKM_Del_Model.jpg">
    </div>
  </div>
</section>
<!-- End Model-->



<!-- Recongtion Gap-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
<h2 class="title is-3">Qualiative results</h2>
<img src="assets/MMKM_Del_QualResult.jpg" alt="Recognition gap frequency distribution for Easy, Hard, and combined sets">
      <h2 class="subtitle has-text-centered">
         We present qualitative results demonstrating the effectiveness of our DEL framework in comparison to unimodal baselines. For the first example, our model, leveraging audio and visual streams, accurately localizes events such as wind noise, cars passing by, driving a motorcycle, and skidding, even in scenarios with overlapping or short-duration occurrences. In contrast, the audio-only model struggles with visually-driven events like skidding, while the vision-only model fails to detect sound-based events like wind noise. Moreover, relying solely on audio leads to incorrect predictions, such as misclassifying the scene as engine knocking due to the absence of visual context. Similarly, the vision-only model, lacking critical audio cues, misinterprets the scene as auto racing from start to finish, based purely on visual perception. This shows the impact of audio in disambiguating visually similar activities. Distinguishing between "man speaking" and another potential sound source in the third scenario is only possible with audio input, as visual information alone is insufficient. Similarly, in the last example,  the scene is crowded, making it challenging to infer "people cheering" using only visual cues. The vision-only model struggles to recognize this category, while the audio modality provides crucial information for its correct identification. Additionally, detecting "people slapping" relies primarily on visual cues. These results highlight how integrating audio and visual streams leads to more accurate and robust event localization, particularly in complex multimodal scenarios.
      </h2>
    </div>
  </div>
</section>
<!-- End Recongtion Gap-->

<!-- Spotlight Presentation -->
<!--
  <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Spotlight</h2>

      <iframe  src="https://andrewjohngilbert.github.io/DEL_ICCV/assets/HumanvsMachineMindsSpotlight.pdf" width="100%" height="1000">
          </iframe>
        
      </div>
    </div>
  </section>
-->
  <!--End Spotlight Presentation -->

<!-- Paper poster -->
<!--
  <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="assets/HumanvsMachineMindsPoster.pdf" width="100%" height="1000">
          </iframe>
        
      </div>
    </div>
  </section>
-->
  <!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <button class="button is-small is-info" onclick="copyBibtex()">Copy BibTeX</button>
<pre id="bibtex-entry"><code>@inproceedings{Ahmadian:DEL:ICCVWS:2025,
  AUTHOR = "Mona Ahmadian and Amir Shirian and Frank Guerin and Andrew Gilbert",
  TITLE = "DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding",
  BOOKTITLE = "International Conference on Computer Vision (ICCV'25) - The 4th Workshop on What is Next in Multimodal Foundation Models? (MMFM4)",
  YEAR = "2025"
}</code></pre>
    <script>
    function copyBibtex() {
      var bib = document.getElementById('bibtex-entry').innerText;
      navigator.clipboard.writeText(bib);
    }
    </script>
  </div>
</section>

<!--End BibTex citation -->




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            <strong>DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding</strong> by 
            <a href="https://www.linkedin.com/in/mona-ahmadian-57853521a/" target="_blank">Mona Ahmadian</a>, 
            <a href="https://uk.linkedin.com/in/amir-shirian-6675587b" target="_blank">Amir Shirian</a>, 
            <a href="https://www.surrey.ac.uk/people/frank-guerin" target="_blank">Frank Guerin</a>, and 
            <a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a>.<br>
            <em>International Conference on Computer Vision (ICCV'25) - The 4th Workshop on What is Next in Multimodal Foundation Models? (MMFM4)</em>
            <br>
            <br>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the design of this website; we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
